###############################################################################
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml-py
#
###############################################################################

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code.
#
# ruff: noqa: E501,F401
# flake8: noqa: E501,F401
# pylint: disable=unused-import,line-too-long
# fmt: off

file_map = {
    
    "citation_extractor.baml": "\n\n// Create a function to extract the resume from a string.\nfunction ExtractCitation(document: Document) -> CitationExtractor {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client \"CustomReasoner\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    Extract dataset citations ONLY from this content. Do not include any other types of citations. Any citation included must specifically mention a dataset.\n    ONLY provide the citation, do noy include any other texts. \n    Example:\n        {\n        \"data_citation\": \"E-GEOD-48278\",\n        \"doc_id\": \"10.1371_journal.pone.0212669\",\n        \"pages\": [\n            1,\n            3\n        ]\n    }\n    If a citation is found on multiple pages, provide all the pages where it is mentioned as an array. The page number associated with each page of the document is provided below:\n\n    {% for page in document.full_text %}\n     --- Page Number {{ loop.index }}:\n     {{ page }}\n     ---\n     {% endfor %}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest get_citation_entities {\n  functions [ExtractCitation]\n  args {\n    document {\n      doi \"10.1101_2022.07.21.501061\"\n      full_text [\n                    #\"bioRxiv preprint\\n\\ndoi:\\n\\nhttps://doi.org/10.1101/2022.07.21.501061\\n\\n;\\n\\nthis version posted July 22, 2022.\\n\\nThe copyright holder for this preprint (which\\n\\nwas not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made\\n\\navailable under a\\n\\nCC-BY-NC 4.0 International license .\\n\\n1\\n\\nClimate Ecology as a Driver of Global Breeding Periods in Anurans\\n\\n2\\n\\n3\\n\\nBryan H. Juarez and Lauren A.\"#,\n                    #\"\n                      .\\n\\nData availability Repository details and associated metadata for curated samples can be found in Supplementary file 1. MTIII and other element measurement data are in Figure 2\\u2014source data 1, and the Rscript used for morphometric analysis is in the DRYAD database (Heintzman et al., 2017). MtDNA genome sequences have been deposited in Genbank under accessions KT168317-KT168336, MF134655- MF134663, and an updated version of JX312727. All mtDNA genome alignments (in NEXUS format) and associated XML and TREE files are in the DRYAD database (Heintzman et al., 2017).\n                    \"#,\n                    #\"\n                      Raw shot- gun sequence data used for the nuclear genomic analyses and raw shotgun and target enrichment sequence data for TMM 34\\u20132518 (francisci holotype) have been deposited in the Short Read Archive (BioProject: PRJNA384940).\\n\\nNomenclatural act The electronic edition of this article conforms to the requirements of the amended International Code of Zoological Nomenclature, and hence the new name contained herein is available under that Code from the electronic edition of this article. This published work and the nomenclatural act it contains have been registered in ZooBank, the online registration system for the ICZN. The ZooBank LSIDs (Life Science Identifiers) can be resolved and the associated information viewed through any standard web browser by appending the LSID to the prefix \\u2018http://zoobank.org/\\u2019. The LSID for this publication is: urn:lsid:zoobank.org:pub:8D270E0A-9148-4089-920C-724F07D8DC0B. The electronic edition of this work was published in a journal with an ISSN, and has been archived and is available from the following digital repositories: PubMed Central and LOCKSS.\\n\\nAcknowledgements We thank the Klondike placer gold mining community of Yukon for their support and providing access to their mines from which many of our Haringtonhippus fossils were recovered.\n                    \"#\n      ]\n      n_pages 3\n      total_char_length 10000\n      total_tokens 1000\n      file_hash \"1234567890\"\n      file_path \"path/to/file.pdf\"\n      parsed_timestamp \"2025-07-13\"\n    }\n  }\n}\n",
    "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\nclient<llm> CustomGPT4oMini {\n  provider openai\n  retry_policy Exponential\n  options {\n    model \"gpt-4o-mini\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGemini {\n  provider google-ai\n  retry_policy Constant\n  options {\n    model \"gemini-2.0-flash\"\n    api_key env.GEMINI_API_KEY\n        generationConfig {\n      temperature 0.5\n    }\n  }\n}\n\nclient<llm> CustomReasoner {\n  provider openai\n  retry_policy Exponential\n  options {\n    model \"o4-mini\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> MyLocalLLM {\n  provider ollama\n  options {\n    // base_url \"http://localhost:11434/v1\"\n    model \"Phi4\"\n  }\n}\n\nclient<llm> CustomSonnet {\n  provider anthropic\n  options {\n    model \"claude-3-5-sonnet-20241022\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n\nclient<llm> CustomHaiku {\n  provider anthropic\n  retry_policy Constant\n  options {\n    model \"claude-3-haiku-20240307\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\nclient<llm> CustomFast {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [CustomGPT4oMini, CustomHaiku]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [CustomGPT4oMini, CustomGPT4oMini]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  // Strategy is optional\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  // Strategy is optional\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}",
    "context.baml": "template_string DataCitationPatterns() #\"\n    \n\n\n\"#\n",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.88.0\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n",
    "models.baml": "class CitationEntity {\n    data_citation string @description(\"Data citation from text\")\n    doc_id string @description(\"DOI of the document where the data citation is found\")\n    pages int[] @description(\"List of page numbers where the data citation is mentioned.\")\n}\n\nclass CitationExtractor {\n    citation_entities CitationEntity[] @description(\"List of dataset citation entities found in the document\")\n    evidence string @description(\"Evidence in the text for the choosing the extracted dataset citation entities\")\n}\n\nclass Document {\n    doi string @description(\"DOI of the document\")\n    has_dataset_citation bool? @description(\"Whether the document has 1 or more dataset citation\")\n    full_text string[] @description(\"Full text of the document\")\n    total_char_length int @description(\"Total number of characters in the document\")\n    parsed_timestamp string @description(\"Timestamp of when the document was parsed\")\n    total_tokens int @description(\"Total number of tokens in the document\")\n    avg_tokens_per_chunk float? @description(\"Average number of tokens per chunk in the document\")\n    file_hash string @description(\"Hash of the document file\")\n    file_path string @description(\"Path to the document file\")\n    n_pages int @description(\"Number of pages in the document\")\n    citation_entities CitationEntity[]? @description(\"List of citation entities found in the document\")\n}\n\n",
}

def get_baml_files():
    return file_map