# conf/chunking.yaml
splitter: semantic          # switch from "sliding" to "semantic"
max_tokens: 300             # hard ceiling to prevent jumbo chunks
similarity_threshold: 0.75  # cosine drop that triggers a new chunk
min_tokens: 80              # merge stragglers below this size
overlap_sentences: 2        # no duplicated text â†’ lower token bill

# Embedding model configuration
# For OpenAI models: text-embedding-3-small, text-embedding-3-large, text-embedding-ada-002
# For offline models: bge-small-en-v1.5, all-MiniLM-L6-v2, all-mpnet-base-v2
# Use offline: prefix for explicit offline models: offline:bge-small-en-v1.5
embed_model: text-embedding-3-small

# Offline model configuration (used when embed_model is offline)
offline_model:
  # Default offline model for when OpenAI is not available
  default_model: bge-small-en-v1.5
  # Cache directory for offline models
  cache_dir: ./offline_models
  # Whether to download model if not cached
  auto_download: true

vector_store:
  type: chroma
  path: ./local_chroma
