# **Compatibility and Design Updates for the Chunking & Embedding Microservice**

## **1\. Preserve RF Classifier Input Schema and Features**

We will **not change the structure of the Random Forest classifier inputs** – all required fields and features will remain intact. In particular, the outcome labels and engineered features must continue to be included in the model inputs:

* **Outcome variables:** The boolean fields `has_data_citation` (for documents) and `is_primary` (for dataset citations) serve as the classification targets and **must remain part of the input schema**. We will ensure these are preserved when constructing the `FirstClassifierInput` and `SecondClassifierInput` objects for the classifiers.

* **UMAP/PCA features:** The dimensionality-reduction features (`UMAP_1`, `UMAP_2`, `PC_1`, `PC_2`) are currently included in the Pydantic models for classifier input. We will continue to generate these features in the later pipeline phase (after embeddings are computed) so that the classifier sees the same inputs as before. In other words, **the pipeline outputs will be compatible with the existing Random Forest’s expected input schema**, only now the data will be fed through our new DuckDB/Chroma-based workflow.

By keeping these fields in place, the downstream Random Forest model can function without any changes. We recognize the *“critical constraint”* that downstream phases expect the exact schema as originally designed, so our modifications will not alter any field names or types required by the classifiers.

## **2\. Updating `run_semantic_chunking.py` to Use DuckDB for I/O**

To integrate DuckDB, we will refactor the semantic chunking pipeline to **read inputs from** and **write outputs to** the database instead of local JSON/CSV files. This aligns with the project goal of running everything offline in Docker with a temporary DuckDB file as the data hub. The changes include:

* **Loading input from DuckDB:** Instead of `load_input_data` reading JSON files, the script will connect to the DuckDB database and query the relevant tables. We will create DuckDB tables for **Documents** and **CitationEntities** (which correspond to the Pydantic models `Document` and `CitationEntity`). For example, we might have:

  1. A `documents` table with columns like `doi`, `full_text`, `total_char_length`, etc., capturing the fields of each `Document`.

  2. A `citations` table with fields like `data_citation` (the dataset ID string), `doc_id` (DOI of the document), `pages`, and `evidence` – reflecting the `CitationEntity` model.

* The chunking script will run a **SELECT query on DuckDB** to fetch all documents and their associated citation entities. We can either fetch the citations separately and group them by `doc_id` in Python (similar to the current code’s behavior), or use a SQL join. Given that the Pydantic `Document` already contains an optional list of citation entities, another approach is to store citation lists in a JSON column in the `documents` table. However, keeping a separate `citations` table is more normalized and flexible. We’ll likely proceed by querying both tables and reconstructing `Document` and `CitationEntity` objects from the results (e.g. using Pydantic’s `model_validate` on each row). This preserves the same in-memory structures as the JSON approach, just sourced from DuckDB.

* **Writing output chunks to DuckDB:** After chunking and embedding, we will **replace the JSON/CSV export with writes to DuckDB tables**. The original pipeline exported two files: `chunks_for_embedding.json` (the list of Chunk objects) and `chunks_for_embedding_summary.csv` (per-chunk stats). Accordingly, we will create **two tables** in DuckDB to store these results:

  1. **`chunks` table:** Stores each chunk with its text and metadata. Each row corresponds to a Pydantic `Chunk` instance. Key columns would include `chunk_id` (unique chunk identifier), `document_id` (DOI of source doc), `text` (chunk text), and token count. We will also store links to adjacent chunks (`previous_chunk_id`, `next_chunk_id`) and any citation IDs contained in the chunk. Because a single chunk can contain **multiple dataset citations**, we need to handle the `citation_entities` list. One approach is to flatten this list into a **JSON or delimited string column** (as the current Chroma metadata does by joining IDs). This way, the entire `Chunk` (including its citations) can be reconstructed from one row – maintaining compatibility with the Pydantic model structure. Alternatively, we could have a separate mapping table (chunk-to-citation), but that adds complexity for little gain at this stage. Storing the citation list as JSON in a column (or as a string of concatenated IDs) ensures the table remains **compatible with the Pydantic schema**, since we can parse that field back into a list of `CitationEntity` objects when needed.

  2. **`chunk_stats` table:** Stores summary info per chunk (as in the CSV). This might include `chunk_id`, `token_count`, `text_length`, and perhaps a text preview or other diagnostic fields. However, these summary fields are mostly derivable from the main `chunks` table (e.g., text length can be `LEN(text)`). We may decide to omit a separate stats table and compute such summaries on the fly via SQL if needed. But since the existing pipeline explicitly outputs a summary CSV, we can mirror that by populating a `chunk_stats` table for completeness. This table would not contain any deeply nested data – just simple columns for counts and lengths – making it straightforward to create with DuckDB SQL (e.g., using `INSERT ... SELECT` from the `chunks` table).

By writing chunks to DuckDB, **the downstream components can query the results directly** instead of reading files. This fosters a seamless hand-off: for instance, the next phase (embedding similarity and dataset aggregation) can select from the `chunks` table to gather needed data. It also conforms to the idea of using a single **in-container database** for all intermediate data, avoiding path issues and keeping everything self-contained in Docker.

## **3\. Consistent Use of Pydantic Models in API and Pipeline**

We will strive to **reuse the same Pydantic models (`src.models.py`) throughout the pipeline and API** to maintain a single source of truth for data schemas. This means the API endpoints (in the new microservice) can directly accept or return objects of type `Document`, `Chunk`, `Dataset`, etc., as defined in the code, rather than defining duplicate classes. Reusing these models has several benefits:

* **Structural consistency:** If the `Document` or `Chunk` model gains a new field or changes type, using the same class everywhere ensures the API and pipeline automatically stay in sync. This avoids the risk of one part of the code expecting a slightly different JSON structure than another – a scenario that could easily happen if we maintained separate but “similar” model definitions in different modules. The user rightly pointed out that having different models for the same logical entity can lead to hard-to-track bugs as the code evolves. By **centralizing on the Pydantic models**, we ensure robust long-term compatibility across components.

* **Convenient serialization:** Pydantic models can be easily serialized to dict/JSON (`model_dump()` or `.json()`), which is handy both for storing in DuckDB (we might store entire objects or sub-objects as JSON) and for returning API responses. FastAPI, for example, can take a Pydantic model in a response and automatically produce JSON. We will leverage this to keep the API implementation clean and aligned with the data structures we’re using internally.

That said, if we encounter any **minor incompatibility** between the Pydantic models and an external interface (for example, maybe the API needs a slightly different field naming or excludes some internal fields), we will **not create a brand-new model**. Instead, we’ll add helper methods or properties to the existing model classes to transform them as needed. For instance, a `Dataset` model could have a method `.to_api_dict()` that drops or renames certain fields for output, if necessary. This way, the transformation logic lives with the model, making it clear and easy to update if the model changes. In summary, we’ll adhere to the principle of **“single definition of data entities”** across the codebase. This approach mitigates structural drift and is aligned with the user’s preference to keep things uniform.

*(As an aside, the Kaggle phase descriptions explicitly warn not to alter the expected data schemas in Phase 6 and beyond. By using the same Pydantic definitions for API and internal steps, we effectively guarantee compliance with the expected schema when it comes time to produce the final outputs.)*

## **4\. Building Dataset-Level Features and Deciding on Embedding Storage**

After chunking and embedding, the next major step is to **construct dataset-level records** for the second classifier (Primary vs Secondary classification). Each dataset record corresponds to a unique combination of a dataset ID and a document (i.e. a specific data citation instance in a paper). We need to gather features for these dataset citations, which includes: the text context, the embedding (or embedding-derived features), and eventually dimensionality reduction or clustering features. Here’s our plan and considerations:

**Aggregating chunks by dataset ID:** For each dataset citation (identified by a dataset\_id in a given document), we will compile the textual evidence from the chunks. Originally, the `Dataset` Pydantic model’s `text` field was just “the text in the document where the dataset citation is found” – often this might be a single sentence or paragraph around the citation. However, the user has specified that we should concatenate *“all chunks found to be associated with this dataset ID”* to form a more comprehensive text. In practice, that means: if a dataset is mentioned in multiple chunks (or even multiple places across the paper), we concatenate those chunk texts into one larger passage for that dataset instance. Furthermore, as part of the similarity search enhancement, we will also include **neighboring chunks** (chunks that don’t explicitly mention the dataset but are topically similar) in this aggregation. In other words, for each dataset citation, we find the top K most similar chunks (using embedding similarity) from anywhere in the corpus and include their content as additional context.

This approach implies some chunks will be used in multiple dataset aggregations. For example, if chunk X is very similar to chunk Y (which contains dataset A) it might be pulled in as a neighbor for dataset A; chunk X might also contain dataset B itself or be similar to another chunk with dataset B, thus chunk X could end up associated with dataset B as well. We acknowledge that **one chunk can be associated with many dataset IDs** in these groupings (either because it explicitly contains multiple citations, or because it’s a neighbor to chunks from different datasets). The pipeline will account for this by not “locking” a chunk to a single dataset context. Instead, association is determined dynamically during the grouping step based on content. This is why our chunk storage design allows a chunk to list multiple citation IDs – it preserves the information needed to know which dataset(s) it directly mentions. For neighbor relationships (chunks that don’t contain the dataset ID but are brought in via similarity), we’ll handle those in code during dataset construction (they won’t be added to the citation list of the chunk itself, but will be added to the aggregated text for that dataset).

**Avoiding duplicate embedding storage:** A critical design question is where to store and retrieve embeddings so that when we assemble these dataset texts and their features, we do not recompute or duplicate data unnecessarily. We have two main options for storing chunk embeddings: using **DuckDB** or using **ChromaDB** (a dedicated vector database). Let’s weigh them:

* **Option 1: Store embeddings in DuckDB (only)** – DuckDB can store arbitrary binary or array data in cells (for example, a vector of floats). We could have an `embeddings` table or add a column in the `chunks` table for the embedding vector. The advantage is all data (text and vectors) lives in one place (the DuckDB file), simplifying data management and portability. However, **DuckDB is not optimized for similarity search** on high-dimensional vectors. We would likely have to load all vectors into memory and perform brute-force distance calculations in Python, or use a DuckDB Python UDF to compute distances for a given query – either way, this could be slow if the number of chunks is large. DuckDB does not (as of now) have a built-in index for vector similarity queries, so it would be a full table scan to find nearest neighbors. This might be acceptable for a modest corpus (e.g., a few thousand chunks), but it will **scale poorly** and increase latency in the pipeline. Another downside is that storing many high-dimensional vectors in the database can bloat its size and possibly slow down other queries. In short, while possible, using DuckDB alone for embeddings trades off query performance and simplicity of KNN retrieval.

* **Option 2: Use ChromaDB (only)** – ChromaDB is designed as a **vector store** with fast similarity search indexes. Our pipeline already includes code to use Chroma: in the original script, after creating chunks, it calls `save_chunk_objs_to_chroma(chunks, ...)` to embed the chunk texts and persist them in a Chroma collection. The benefits here are clear: we get efficient nearest-neighbor lookup by embedding, and Chroma can store metadata with each vector (like chunk\_id, document\_id, etc.) for filtering or identification. In fact, our current approach uses metadata to filter by document if needed and stores the chunk text as well. We could conceivably use Chroma as the **sole** store for chunks and their data – it can return the stored documents (chunk texts) and metadata – so maybe we wouldn’t need DuckDB at all for chunk storage. However, relying only on Chroma has drawbacks: it’s not a relational database, so operations like joining with other data (e.g., linking with document-level info or labels) or performing aggregations/statistics would be clunky. We’d likely end up loading a lot of data out of Chroma into Python to manipulate it, which negates some benefits. Moreover, while Chroma is great for similarity queries, it’s not as convenient for structured queries that DuckDB excels at (for example, “group all chunks by document\_id” or “count chunks per document”). We also have to ensure Chroma runs **completely offline in the Docker container**. Chroma does support an in-memory or local mode (and we are indeed using a persistent local directory for it in this project), so that part is fine for the competition constraints.

* **Option 3: Hybrid (DuckDB \+ Chroma)** – This is the approach we are leaning towards, as it plays to each technology’s strengths. In this setup, DuckDB remains the system of record for **structured data** (documents, chunks metadata, dataset aggregations, labels, etc.), and **ChromaDB is used for the heavy lifting of vector similarity search**. We will not duplicate the entire embedding in both; instead:

  * **DuckDB** will store chunk identifiers and any necessary fields *except* the high-dim embedding. For example, the `chunks` table can have a primary key (chunk\_id) and textual fields and stats, and perhaps a boolean flag if an embedding exists.

  * **ChromaDB** will store each chunk’s embedding vector along with a minimal set of metadata (chunk\_id, document\_id, etc.). In fact, our current `save_chunk_objs_to_chroma` already upserts each chunk with its `chunk_id` as the Chroma ID and includes metadata like `document_id` and a concatenated list of citation IDs. We will continue using that.

* The pros of this hybrid approach: we avoid storing large vectors in DuckDB (keeping the DB lean and fast for SQL operations), and we leverage Chroma’s optimized similarity search to quickly retrieve *top-K similar chunks*. When it’s time to construct a Dataset’s text and features, we can do something like: for each dataset ID, find all chunks that explicitly mention it (from DuckDB, via the chunk’s citation info), then query Chroma for nearest neighbors of those chunks’ embeddings. Chroma can give us the IDs of similar chunks and their similarity scores. With those IDs, we can fetch the actual chunk text (and any other details if needed) from DuckDB (or we could store the text in Chroma and get it directly – we already store chunk text as the “document” in Chroma, so we might even get it directly in the Chroma query result). After gathering the relevant chunks, we concatenate their texts to form the dataset’s full context and create the `Dataset` object.

   The **con** of using both systems is the added complexity of maintenance – we have to ensure whenever we add a chunk, we insert into DuckDB and Chroma in sync. Fortunately, our pipeline already does this in one step (it creates chunks in memory, then calls Chroma upsert; we will add a step to also insert into DuckDB within the same run). The systems are local and embedded, so it doesn’t introduce external service dependencies, just two libraries. Given the scale of this problem, the overhead is minimal and worth the trade-off. This design aligns with the project’s plan: the **competition guidelines explicitly mention using ChromaDB for vector storage** and DuckDB for intermediate results. We will follow that blueprint, as it provides efficient retrieval while keeping our data well-organized.

**Pros and Cons Summary:**

* *DuckDB-only (no Chroma):* Simple single-file solution and easy integration with other data, but no built-in similarity index – would require custom Python loops or full scans for KNN, which is inefficient. Harder to scale and possibly large DB size if storing many vectors.

* *Chroma-only (no DuckDB):* Excellent for similarity queries and can hold text+metadata, but not designed for complex filtering or joining on non-vector fields. We’d lose the SQL power of DuckDB for things like quickly counting, grouping, or merging with labels. Also, we would need to output final results from Chroma to, say, a CSV for submission anyway, so a DB like DuckDB is helpful for final assembly.

* *DuckDB \+ Chroma (hybrid):* Leverages the strength of each – DuckDB for structured ops and data integrity, Chroma for fast vector search. Slightly more complex, but ensures we **store embeddings in one place only** (Chroma) and can retrieve them easily. No duplicate storage – we will **treat Chroma as the source of truth for embeddings**, and DuckDB as source of truth for everything else. This way, if we need an embedding (or to find similar ones), we query Chroma; if we need attribute data or lists of chunks/documents, we query DuckDB. This combined approach is what we recommend and will implement, as it satisfies both performance and compatibility needs.

In conclusion, we will proceed with DuckDB \+ Chroma. The chunking & embedding microservice will thus: write chunk texts/IDs to DuckDB, and upsert chunk embeddings to Chroma. The subsequent dataset-construction step can then easily do its job by pulling the necessary pieces from these two sources without any redundant computation or storage.

## **5\. Lightweight Docker Image & Dependency Isolation**

We will containerize this **chunking & embedding step** as its own microservice, ensuring the image is as slim as possible and contains **only the needed dependencies** for this specific task. The rationale is to avoid “dependency hell” by isolating each pipeline phase into its own environment, which the user has explicitly requested. Some steps (like PDF parsing in Phase 2\) require heavy libraries that we don’t want cluttering the environment for other steps, and conversely, the ML classification step might have its own set of libraries. By keeping them separate, we prevent version conflicts and reduce the overall build complexity.

Concretely, for the chunking & embedding container we will:

* **Choose a minimal base image** – likely a slim Python image (e.g., `python:3.x-slim`). This reduces image size. We won’t include any GUI or unnecessary OS packages.

* **Install only essential libraries**: Based on our updated script, the core dependencies are:

  * **DuckDB** (the Python package) for database access.

  * **ChromaDB** for vector storage.

  * **OpenAI embedding model** – we use the OpenAI API via `openai` Python package (and ensure the environment variable for the API key is set). This brings in `openai` and its deps (which are fairly lightweight).

  * **llama\_index (GPT Index)** for the semantic text splitter (if we continue using `SemanticSplitterNodeParser`). We have to include this or implement an equivalent. The llama-index library might pull in some dependencies (like `transformers` or others) – we will evaluate if we can slim this down. If the semantic chunking function can be configured to use only OpenAI API calls (which we are doing) and a bit of logic, we might not need the entire llama\_index package. We’ll try to limit this, but if it’s required for the semantic chunking algorithm, we’ll include it.

  * **Pydantic** (likely already installed via our own code or via FastAPI) for data models.

  * **FastAPI and Uvicorn** (if we are exposing an API endpoint). The user mentioned “using uv since I am already using that,” which suggests they are using Uvicorn as the ASGI server (often paired with FastAPI). We will use Uvicorn to run our service, and FastAPI (or a similar lightweight framework) to define an endpoint that triggers the chunking process. FastAPI will also automatically handle Pydantic model serialization in the responses. These are relatively small additions and widely used for Python microservices.

* **Omit unrelated dependencies:** We will not include anything not needed for this phase. For example, libraries for PDF parsing (Unstructured, PyMuPDF, etc.), training libraries (scikit-learn, pandas beyond what’s needed for maybe minor DataFrame usage), or heavy visualization libraries will be left out. This ensures the image remains lean. The Random Forest model training and UMAP/PCA computation will occur in a **different container** later, so those libraries (e.g., scikit-learn, umap-learn, numpy, etc.) will be installed in that other container, not here. By **designing “siloed” microservices**, each Docker image remains focused and smaller, and we avoid version conflicts between, say, an NLP library and an ML library that might require different versions of the same dependency.

* **Validate offline capability:** The container will run entirely offline, which means we’ll download any model files or large resources ahead of time (if any). In our case, we’re using the OpenAI API for embeddings, which in a truly offline scenario would be an issue – however, for the Kaggle competition environment, internet is disabled, so we actually need to switch to a local embedding model or use precomputed embeddings. This is a consideration: since we can’t rely on external API calls in the final Docker, we might integrate an open-source embedding model (like SentenceTransformers) and include it in the image. This would add to dependencies (PyTorch and a model file), but it’s necessary for offline execution. We will choose a small, efficient model to limit the size impact. (If using OpenAI’s API was only for convenience during development, we’ll replace it for the final container with an offline method.) In any case, we ensure **no dependency in the container tries to call home or download at runtime** – everything must be packaged or vendored beforehand, per competition rules.

* **Use pyproject.toml for dependency management:** The user has a pyproject.toml, indicating they might use Poetry or pip-tools. We’ll update that to reflect the precise dependencies for this microservice. This makes building the container reproducible and helps avoid accidentally dragging in extra packages.

Finally, we’ll test the container by running the API and the chunking pipeline on a small sample to ensure it starts up quickly and uses reasonable memory/CPU. The outcome will be a Docker image that can run **just the chunking & embedding step** in isolation. When orchestrating the full pipeline, the idea is that: the parsing container produces its outputs (in DuckDB and perhaps some JSON), then the chunking container (we’re building now) picks up from the DuckDB (or via an API call that triggers it and it reads DuckDB), does its job, stores results back to DuckDB (and Chroma), and so on. This microservice architecture cleanly separates phases and, as noted, prevents the mire of conflicting dependencies in one massive container. The approach is in line with the project’s goal of modular pipeline steps and will make development and debugging easier.

---

**References:**

1. Tavjo et al., *MDC Challenge Code – Pydantic Models*, showing classifier input schemas (including outcome and UMAP/PC features).

2. Tavjo et al., *Logical Flow Documentation*, outlining the pipeline phases and data deliverables (DuckDB usage, ChromaDB for embeddings, and requirement to keep schemas consistent).

3. Tavjo et al., *Chunking Pipeline Code (`run_semantic_chunking.py`)*, illustrating how chunks and citations are handled (ensuring all dataset IDs in a chunk are captured) and how results were originally exported to files.

4. Tavjo et al., *Semantic Chunking with Chroma (`semantic_chunking.py`)*, demonstrating integration with ChromaDB (embedding chunks and storing metadata).

5. Tavjo et al., *Logical Flow Documentation*, Phase 2 note on containerizing parsing for dependency isolation – motivating our microservices approach.

