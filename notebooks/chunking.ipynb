{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86ed3703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/taishajoseph/Documents/Projects/MDC-Challenge-2025\n",
      "Python path includes project root: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/taishajoseph/Documents/Projects/MDC-Challenge-2025')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import dependencies\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "# get project root\n",
    "project_root = Path(os.getcwd()).parent\n",
    "\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Python path includes project root: {str(project_root) in sys.path}\")\n",
    "project_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0e1e184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 20:29:52,938 - src.helpers - INFO - Logging initialized for /Users/taishajoseph/Documents/Projects/MDC-Challenge-2025/logs/semantic_chunking.log\n",
      "2025-07-15 20:29:52,938 - src.helpers - INFO - Logging initialized for /Users/taishajoseph/Documents/Projects/MDC-Challenge-2025/logs/semantic_chunking.log\n"
     ]
    }
   ],
   "source": [
    "from src.semantic_chunking import semantic_chunk_text\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9a4e720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "▸ Loading OpenAI embedding model text-embedding-3-small\n",
      "2025-07-15 20:29:52,952 - src.helpers - INFO - ▸ Loading OpenAI embedding model text-embedding-3-small\n",
      "2025-07-15 20:29:52,952 - src.helpers - INFO - ▸ Loading OpenAI embedding model text-embedding-3-small\n",
      "▸ Splitting text with SemanticSplitterNodeParser (chunk_size=300, τ=0.75)\n",
      "2025-07-15 20:29:52,955 - src.helpers - INFO - ▸ Splitting text with SemanticSplitterNodeParser (chunk_size=300, τ=0.75)\n",
      "2025-07-15 20:29:52,955 - src.helpers - INFO - ▸ Splitting text with SemanticSplitterNodeParser (chunk_size=300, τ=0.75)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing semantic_chunk_text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 20:29:53,900 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-07-15 20:29:53,900 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "▸ Created 1 semantic chunks\n",
      "2025-07-15 20:29:53,958 - src.helpers - INFO - ▸ Created 1 semantic chunks\n",
      "2025-07-15 20:29:53,958 - src.helpers - INFO - ▸ Created 1 semantic chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function semantic_chunk_text took 1.0114 seconds to complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Semantic chunking splits text along **topic changes** instead of raw\\ntoken counts.  This standalone demo embeds the chunks via OpenAI and\\nstores them in a local ChromaDB collection named 'test'.\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = textwrap.dedent(\n",
    "    \"\"\"\n",
    "    Semantic chunking splits text along **topic changes** instead of raw\n",
    "    token counts.  This standalone demo embeds the chunks via OpenAI and\n",
    "    stores them in a local ChromaDB collection named 'test'.\n",
    "    \"\"\"\n",
    ").strip()\n",
    "\n",
    "cks = semantic_chunk_text(sample_text)\n",
    "cks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c4d19d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "▸ Loading OpenAI embedding model text-embedding-3-small\n",
      "2025-07-15 20:29:53,974 - src.helpers - INFO - ▸ Loading OpenAI embedding model text-embedding-3-small\n",
      "2025-07-15 20:29:53,974 - src.helpers - INFO - ▸ Loading OpenAI embedding model text-embedding-3-small\n",
      "▸ Embedding 1 chunks for Chroma upsert\n",
      "2025-07-15 20:29:53,976 - src.helpers - INFO - ▸ Embedding 1 chunks for Chroma upsert\n",
      "2025-07-15 20:29:53,976 - src.helpers - INFO - ▸ Embedding 1 chunks for Chroma upsert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing save_chunks_to_chroma...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 20:29:54,403 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-07-15 20:29:54,403 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-07-15 20:29:54,429 - chromadb.telemetry.product.posthog - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-07-15 20:29:54,429 - chromadb.telemetry.product.posthog - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "▸ Upserting 1 items into collection 'test' (path=./local_chroma)\n",
      "2025-07-15 20:29:54,518 - src.helpers - INFO - ▸ Upserting 1 items into collection 'test' (path=./local_chroma)\n",
      "2025-07-15 20:29:54,518 - src.helpers - INFO - ▸ Upserting 1 items into collection 'test' (path=./local_chroma)\n",
      "✅ Persisted collection to disk (./local_chroma)\n",
      "2025-07-15 20:29:54,713 - src.helpers - INFO - ✅ Persisted collection to disk (./local_chroma)\n",
      "2025-07-15 20:29:54,713 - src.helpers - INFO - ✅ Persisted collection to disk (./local_chroma)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function save_chunks_to_chroma took 0.7446 seconds to complete.\n"
     ]
    }
   ],
   "source": [
    "from src.semantic_chunking import save_chunks_to_chroma\n",
    "save_chunks_to_chroma(cks, collection_name=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53341bf",
   "metadata": {},
   "source": [
    "## Now try to run semantic chunking on a subset of corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ed05884",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Logging initialized for /Users/taishajoseph/Documents/Projects/MDC-Challenge-2025/logs/run_semantic_chunking.log\n",
      "2025-07-15 20:29:54,754 - src.helpers - INFO - Logging initialized for /Users/taishajoseph/Documents/Projects/MDC-Challenge-2025/logs/run_semantic_chunking.log\n",
      "2025-07-15 20:29:54,754 - src.helpers - INFO - Logging initialized for /Users/taishajoseph/Documents/Projects/MDC-Challenge-2025/logs/run_semantic_chunking.log\n",
      "2025-07-15 20:29:54,754 - src.helpers - INFO - Logging initialized for /Users/taishajoseph/Documents/Projects/MDC-Challenge-2025/logs/run_semantic_chunking.log\n"
     ]
    }
   ],
   "source": [
    "from src.run_semantic_chunking import run_semantic_chunking_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4ab468b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Semantic Chunking Pipeline ===\n",
      "2025-07-15 20:29:54,765 - src.helpers - INFO - === Semantic Chunking Pipeline ===\n",
      "2025-07-15 20:29:54,765 - src.helpers - INFO - === Semantic Chunking Pipeline ===\n",
      "2025-07-15 20:29:54,765 - src.helpers - INFO - === Semantic Chunking Pipeline ===\n",
      "Started at: 2025-07-15 20:29:54.770436\n",
      "2025-07-15 20:29:54,770 - src.helpers - INFO - Started at: 2025-07-15 20:29:54.770436\n",
      "2025-07-15 20:29:54,770 - src.helpers - INFO - Started at: 2025-07-15 20:29:54.770436\n",
      "2025-07-15 20:29:54,770 - src.helpers - INFO - Started at: 2025-07-15 20:29:54.770436\n",
      "\n",
      "📋 Step 1: Load Input Data\n",
      "2025-07-15 20:29:54,773 - src.helpers - INFO - \n",
      "📋 Step 1: Load Input Data\n",
      "2025-07-15 20:29:54,773 - src.helpers - INFO - \n",
      "📋 Step 1: Load Input Data\n",
      "2025-07-15 20:29:54,773 - src.helpers - INFO - \n",
      "📋 Step 1: Load Input Data\n",
      "Loading documents from: Data/train/documents_with_known_entities.json\n",
      "2025-07-15 20:29:54,776 - src.helpers - INFO - Loading documents from: Data/train/documents_with_known_entities.json\n",
      "2025-07-15 20:29:54,776 - src.helpers - INFO - Loading documents from: Data/train/documents_with_known_entities.json\n",
      "2025-07-15 20:29:54,776 - src.helpers - INFO - Loading documents from: Data/train/documents_with_known_entities.json\n",
      "Loading citation entities from /Users/taishajoseph/Documents/Projects/MDC-Challenge-2025/Data/train/documents_with_known_entities.json\n",
      "2025-07-15 20:29:54,780 - src.helpers - INFO - Loading citation entities from /Users/taishajoseph/Documents/Projects/MDC-Challenge-2025/Data/train/documents_with_known_entities.json\n",
      "2025-07-15 20:29:54,780 - src.helpers - INFO - Loading citation entities from /Users/taishajoseph/Documents/Projects/MDC-Challenge-2025/Data/train/documents_with_known_entities.json\n",
      "2025-07-15 20:29:54,780 - src.helpers - INFO - Loading citation entities from /Users/taishajoseph/Documents/Projects/MDC-Challenge-2025/Data/train/documents_with_known_entities.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing run_semantic_chunking_pipeline...\n",
      "Executing load_input_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading citation entities from: Data/citation_entities_known.json\n",
      "2025-07-15 20:29:55,001 - src.helpers - INFO - Loading citation entities from: Data/citation_entities_known.json\n",
      "2025-07-15 20:29:55,001 - src.helpers - INFO - Loading citation entities from: Data/citation_entities_known.json\n",
      "2025-07-15 20:29:55,001 - src.helpers - INFO - Loading citation entities from: Data/citation_entities_known.json\n",
      "✅ Loaded 524 documents and 487 citation entities\n",
      "2025-07-15 20:29:55,018 - src.helpers - INFO - ✅ Loaded 524 documents and 487 citation entities\n",
      "2025-07-15 20:29:55,018 - src.helpers - INFO - ✅ Loaded 524 documents and 487 citation entities\n",
      "2025-07-15 20:29:55,018 - src.helpers - INFO - ✅ Loaded 524 documents and 487 citation entities\n",
      "\n",
      "📋 Step 2: Create Pre-Chunk Entity Inventory\n",
      "2025-07-15 20:29:55,035 - src.helpers - INFO - \n",
      "📋 Step 2: Create Pre-Chunk Entity Inventory\n",
      "2025-07-15 20:29:55,035 - src.helpers - INFO - \n",
      "📋 Step 2: Create Pre-Chunk Entity Inventory\n",
      "2025-07-15 20:29:55,035 - src.helpers - INFO - \n",
      "📋 Step 2: Create Pre-Chunk Entity Inventory\n",
      "Creating pre-chunk entity inventory for 2 documents\n",
      "2025-07-15 20:29:55,038 - src.helpers - INFO - Creating pre-chunk entity inventory for 2 documents\n",
      "2025-07-15 20:29:55,038 - src.helpers - INFO - Creating pre-chunk entity inventory for 2 documents\n",
      "2025-07-15 20:29:55,038 - src.helpers - INFO - Creating pre-chunk entity inventory for 2 documents\n",
      "\n",
      "❌ Pipeline failed: No matching document IDs between documents and citations\n",
      "2025-07-15 20:29:55,041 - src.helpers - ERROR - \n",
      "❌ Pipeline failed: No matching document IDs between documents and citations\n",
      "2025-07-15 20:29:55,041 - src.helpers - ERROR - \n",
      "❌ Pipeline failed: No matching document IDs between documents and citations\n",
      "2025-07-15 20:29:55,041 - src.helpers - ERROR - \n",
      "❌ Pipeline failed: No matching document IDs between documents and citations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function load_input_data took 0.2482 seconds to complete.\n",
      "Executing create_pre_chunk_entity_inventory...\n",
      "Function run_semantic_chunking_pipeline took 0.2788 seconds to complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChunkingResult(success=False, total_documents=2, total_unique_datasets=487, total_chunks=0, total_tokens=0, avg_tokens_per_chunk=0.0, validation_passed=False, pipeline_completed_at='2025-07-15T20:29:55.044669', entity_retention=0.0, output_path=None, output_files=None, lost_entities=None, error='No matching document IDs between documents and citations')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_semantic_chunking_pipeline(subset=True, subset_size=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdc-challenge-2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
