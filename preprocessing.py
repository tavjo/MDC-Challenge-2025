# preprocessing.py

"""
Preprocessing Pipeline Orchestrator

This script provides a comprehensive preprocessing pipeline for the Make Data Count challenge.
It includes step management, flexible execution methods, CLI interface, and consolidated reporting.

Features:
- Step-by-step execution with dependency management
- Flexible execution modes (run all, specific steps, up to step, from step)
- CLI interface with comprehensive argument parsing
- Consolidated reporting with JSON and Markdown outputs
- Error handling and recovery mechanisms
- Progress tracking with timing and metadata collection

Steps to be run:
1) Pre-Chunking EDA (scripts/run_prechunking_eda.py)
2) PDF -> XML conversion (scripts/run_doc_conversion.py)
3) Document Parsing (scripts/run_full_doc_parsing.py)
4) Semantic Chunking (scripts/run_chunking_pipeline.py)
5) Create Vector Embeddings 
6) Chunk-level EDA
7) QC
8) Export Artifacts for training loop
9) Generate Report
"""

import sys
import os
import json
import argparse
from pathlib import Path
from typing import Dict, List, Optional, Any, Set
from datetime import datetime
import pickle
from enum import StrEnum
import psutil
import time
import shutil
from pydantic import BaseModel, Field, field_validator

# Add current directory to path
sys.path.append(str(Path(__file__).parent))

from src.helpers import initialize_logging, timer_wrap

filename = os.path.basename(__file__)
logger = initialize_logging(filename)


class StepStatus(StrEnum):
    """Step execution status."""
    NOT_STARTED = "not_started"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"


class ErrorType(StrEnum):
    """Error categorization."""
    RECOVERABLE = "recoverable"
    FATAL = "fatal"
    TRANSIENT = "transient"


class ValidationError(Exception):
    """Custom exception for validation errors."""
    pass


class ResourceAlert(BaseModel):
    """Resource usage alert."""
    alert_type: str = Field(..., description="Type of alert (memory, disk, cpu)")
    threshold: float = Field(..., description="Threshold that was exceeded")
    current_value: float = Field(..., description="Current resource usage value")
    message: str = Field(..., description="Alert message")
    timestamp: datetime = Field(..., description="When the alert occurred")


class PerformanceMetrics(BaseModel):
    """Performance metrics for a step."""
    execution_time: float = Field(..., description="Execution time in seconds")
    memory_usage: Dict[str, float] = Field(default_factory=dict, description="Memory usage statistics")
    cpu_usage: float = Field(default=0.0, description="CPU usage percentage")
    disk_usage: Dict[str, float] = Field(default_factory=dict, description="Disk usage statistics")
    file_sizes: Dict[str, float] = Field(default_factory=dict, description="File sizes in bytes")
    throughput: Optional[float] = Field(None, description="Processing throughput (items/second)")


class StepMetadata(BaseModel):
    """Metadata for each preprocessing step."""
    name: str = Field(..., description="Name of the preprocessing step")
    description: str = Field(..., description="Description of what the step does")
    status: StepStatus = Field(..., description="Current execution status of the step")
    start_time: Optional[datetime] = Field(None, description="When the step started execution")
    end_time: Optional[datetime] = Field(None, description="When the step finished execution")
    duration: Optional[float] = Field(None, description="Duration of step execution in seconds")
    error_message: Optional[str] = Field(None, description="Error message if step failed")
    error_type: Optional[ErrorType] = Field(None, description="Type of error that occurred")
    retry_count: int = Field(default=0, description="Number of retry attempts")
    parameters: Optional[Dict[str, Any]] = Field(None, description="Parameters used for step execution")
    output_files: Optional[List[str]] = Field(None, description="Files generated by the step")
    metrics: Optional[Dict[str, Any]] = Field(None, description="Metrics collected during step execution")
    performance_metrics: Optional[PerformanceMetrics] = Field(None, description="Performance metrics")
    resource_alerts: List[ResourceAlert] = Field(default_factory=list, description="Resource usage alerts")


class PipelineState(BaseModel):
    """Pipeline state for persistence."""
    current_step: Optional[str] = Field(None, description="Currently executing step")
    completed_steps: List[str] = Field(default_factory=list, description="List of completed steps")
    failed_steps: List[str] = Field(default_factory=list, description="List of failed steps")
    parameters: Dict[str, Any] = Field(default_factory=dict, description="Pipeline parameters")
    start_time: Optional[datetime] = Field(None, description="When the pipeline started")
    last_update: Optional[datetime] = Field(None, description="Last time the state was updated")
    checkpoints: Dict[str, datetime] = Field(default_factory=dict, description="Checkpoint timestamps")
    resource_alerts: List[ResourceAlert] = Field(default_factory=list, description="Pipeline-level resource alerts")
    
    @field_validator('completed_steps', 'failed_steps', mode='before')
    @classmethod
    def validate_step_lists(cls, v):
        """Ensure step lists are not None."""
        return v if v is not None else []
    
    @field_validator('parameters', mode='before')
    @classmethod
    def validate_parameters(cls, v):
        """Ensure parameters dict is not None."""
        return v if v is not None else {}


@timer_wrap
class PreprocessingPipeline:
    """
    Enhanced preprocessing pipeline with step management and flexible execution.
    """
    
    # Step definitions with names, descriptions, and dependencies
    STEP_DEFINITIONS = {
        "pre_chunking_eda": {
            "name": "Pre-Chunking EDA",
            "description": "Exploratory data analysis before chunking",
            "dependencies": [],
            "method": "pre_chunking_eda",
            "default_params": {
                "output_format": "all",
                "save_reports": True,
                "show_plots": False
            }
        },
        "doc_conversion": {
            "name": "Document Conversion",
            "description": "PDF to XML conversion",
            "dependencies": ["pre_chunking_eda"],
            "method": "doc_conversion",
            "default_params": {
                "output_format": "all",
                "save_reports": True
            }
        },
        "document_parsing": {
            "name": "Document Parsing",
            "description": "Full document parsing and extraction",
            "dependencies": ["doc_conversion"],
            "method": "document_parsing",
            "default_params": {
                "output_format": "all",
                "save_reports": True
            }
        },
        "semantic_chunking": {
            "name": "Semantic Chunking",
            "description": "Semantic text chunking for processing",
            "dependencies": ["document_parsing"],
            "method": "semantic_chunking",
            "default_params": {
                "chunk_size": 200,
                "chunk_overlap": 20,
                "output_format": "all",
                "save_reports": True
            }
        },
        "vector_embeddings": {
            "name": "Vector Embeddings",
            "description": "Create vector embeddings from chunks",
            "dependencies": ["semantic_chunking"],
            "method": "create_vector_embeddings",
            "default_params": {}
        },
        "chunk_level_eda": {
            "name": "Chunk-Level EDA",
            "description": "Exploratory data analysis at chunk level",
            "dependencies": ["vector_embeddings"],
            "method": "chunk_level_eda",
            "default_params": {}
        },
        "qc": {
            "name": "Quality Control",
            "description": "Quality control checks",
            "dependencies": ["chunk_level_eda"],
            "method": "qc",
            "default_params": {}
        },
        "export_artifacts": {
            "name": "Export Artifacts",
            "description": "Export artifacts for training loop",
            "dependencies": ["qc"],
            "method": "export_artifacts",
            "default_params": {}
        }
    }
    
    def __init__(self, data_dir: str = "Data", config_file: Optional[str] = None):
        """
        Initialize the preprocessing pipeline.
        
        Args:
            data_dir: Directory containing the data
            config_file: Optional configuration file path
        """
        logger.info("Initializing Enhanced PreprocessingPipeline...")
        
        self.data_dir = data_dir
        self.config_file = config_file
        
        # Initialize step metadata
        self.step_metadata: Dict[str, StepMetadata] = {}
        self._initialize_step_metadata()
        
        # Initialize pipeline state
        self.state = PipelineState()
        self.state_file = Path(data_dir) / "pipeline_state.pkl"
        
        # Initialize step parameters
        self.step_parameters: Dict[str, Dict[str, Any]] = {}
        self._initialize_default_parameters()
        
        # Load configuration if provided
        if config_file:
            self._load_config(config_file)
        
        # Try to load existing state
        self._load_state()
        
        # Initialize resource monitoring
        self.resource_thresholds = {
            'memory_percent': 85.0,  # Alert when memory usage exceeds 85%
            'disk_percent': 90.0,    # Alert when disk usage exceeds 90%
            'cpu_percent': 90.0      # Alert when CPU usage exceeds 90%
        }
        
        # Initialize retry configuration
        self.max_retries = 3
        self.retry_delay = 5  # seconds
        
        logger.info(f"Pipeline initialized with {len(self.STEP_DEFINITIONS)} steps")

    def _initialize_step_metadata(self):
        """Initialize metadata for all steps."""
        for step_id, step_def in self.STEP_DEFINITIONS.items():
            self.step_metadata[step_id] = StepMetadata(
                name=step_def["name"],
                description=step_def["description"],
                status=StepStatus.NOT_STARTED
            )

    def _initialize_default_parameters(self):
        """Initialize default parameters for all steps."""
        for step_id, step_def in self.STEP_DEFINITIONS.items():
            self.step_parameters[step_id] = step_def["default_params"].copy()

    def _load_config(self, config_file: str):
        """Load configuration from file."""
        try:
            with open(config_file, 'r') as f:
                config = json.load(f)
            
            # Update step parameters with config values
            for step_id, params in config.get("step_parameters", {}).items():
                if step_id in self.step_parameters:
                    self.step_parameters[step_id].update(params)
            
            logger.info(f"Configuration loaded from {config_file}")
        except Exception as e:
            logger.warning(f"Failed to load configuration from {config_file}: {e}")

    def _save_state(self):
        """Save current pipeline state."""
        try:
            self.state.last_update = datetime.now()
            with open(self.state_file, 'wb') as f:
                pickle.dump(self.state, f)
            logger.debug(f"Pipeline state saved to {self.state_file}")
        except Exception as e:
            logger.error(f"Failed to save pipeline state: {e}")

    def _load_state(self):
        """Load pipeline state if exists."""
        if self.state_file.exists():
            try:
                with open(self.state_file, 'rb') as f:
                    self.state = pickle.load(f)
                logger.info(f"Pipeline state loaded from {self.state_file}")
            except Exception as e:
                logger.warning(f"Failed to load pipeline state: {e}")
    
    # ================== VALIDATION METHODS ==================
    
    def validate_data_directory_structure(self) -> bool:
        """
        Validate that the data directory has the expected structure.
        
        Returns:
            True if structure is valid, False otherwise
        """
        try:
            data_path = Path(self.data_dir)
            
            # Check if data directory exists
            if not data_path.exists():
                raise ValidationError(f"Data directory {self.data_dir} does not exist")
            
            # Check for required subdirectories
            required_dirs = ['train', 'test']
            for dir_name in required_dirs:
                dir_path = data_path / dir_name
                if not dir_path.exists():
                    logger.warning(f"Expected directory {dir_path} does not exist")
            
            # Check for basic files
            basic_files = ['train/documents', 'test/documents']
            for file_path in basic_files:
                full_path = data_path / file_path
                if not full_path.exists():
                    logger.warning(f"Expected path {full_path} does not exist")
            
            logger.info("Data directory structure validation passed")
            return True
            
        except ValidationError as e:
            logger.error(f"Data directory validation failed: {e}")
            return False
        except Exception as e:
            logger.error(f"Unexpected error during directory validation: {e}")
            return False
    
    def validate_file_dependencies(self, step_id: str) -> bool:
        """
        Validate that all required files exist for a step.
        
        Args:
            step_id: The step to validate dependencies for
            
        Returns:
            True if all dependencies exist, False otherwise
        """
        try:
            data_path = Path(self.data_dir)
            
            # Define file dependencies for each step
            file_dependencies = {
                'pre_chunking_eda': [
                    'train/documents',
                    'test/documents'
                ],
                'doc_conversion': [
                    'train/documents',
                    'test/documents'
                ],
                'document_parsing': [
                    'train/xml',
                    'test/xml'
                ],
                'semantic_chunking': [
                    'train/parsed/parsed_documents.pkl'
                ],
                'vector_embeddings': [
                    'chunks_for_embedding.pkl'
                ],
                'chunk_level_eda': [
                    'chunks_for_embedding.pkl'
                ],
                'qc': [
                    'chunks_for_embedding.pkl'
                ],
                'export_artifacts': [
                    'chunks_for_embedding.pkl'
                ]
            }
            
            if step_id not in file_dependencies:
                logger.warning(f"No file dependencies defined for step {step_id}")
                return True
            
            missing_files = []
            for file_path in file_dependencies[step_id]:
                full_path = data_path / file_path
                if not full_path.exists():
                    missing_files.append(str(full_path))
            
            if missing_files:
                logger.error(f"Missing dependencies for step {step_id}: {missing_files}")
                return False
            
            logger.info(f"File dependencies validation passed for step {step_id}")
            return True
            
        except Exception as e:
            logger.error(f"Error validating file dependencies for step {step_id}: {e}")
            return False
    
    def validate_step_input_format(self, step_id: str) -> bool:
        """
        Validate the format of input data for a step.
        
        Args:
            step_id: The step to validate input format for
            
        Returns:
            True if input format is valid, False otherwise
        """
        try:
            data_path = Path(self.data_dir)
            
            # Validate specific file formats based on step
            if step_id == 'semantic_chunking':
                # Check if parsed_documents.pkl is a valid pickle file
                pkl_path = data_path / 'train/parsed/parsed_documents.pkl'
                if pkl_path.exists():
                    try:
                        with open(pkl_path, 'rb') as f:
                            data = pickle.load(f)
                        logger.info(f"Validated pickle file format for {step_id}")
                    except Exception as e:
                        logger.error(f"Invalid pickle file format: {e}")
                        return False
            
            elif step_id == 'document_parsing':
                # Check if XML files exist and are readable
                xml_dir = data_path / 'train/xml'
                if xml_dir.exists():
                    xml_files = list(xml_dir.glob('*.xml'))
                    if not xml_files:
                        logger.warning(f"No XML files found in {xml_dir}")
                        return False
                    
                    # Test read first few XML files
                    for xml_file in xml_files[:3]:
                        try:
                            with open(xml_file, 'r', encoding='utf-8') as f:
                                content = f.read(100)  # Read first 100 chars
                            if not content.strip().startswith('<?xml'):
                                logger.warning(f"File {xml_file} may not be valid XML")
                        except Exception as e:
                            logger.error(f"Error reading XML file {xml_file}: {e}")
                            return False
            
            logger.info(f"Input format validation passed for step {step_id}")
            return True
            
        except Exception as e:
            logger.error(f"Error validating input format for step {step_id}: {e}")
            return False
    
    # ================== ERROR HANDLING METHODS ==================
    
    def categorize_error(self, error: Exception) -> ErrorType:
        """
        Categorize an error as recoverable, fatal, or transient.
        
        Args:
            error: The exception that occurred
            
        Returns:
            ErrorType classification
        """
        error_str = str(error).lower()
        
        # Transient errors (should be retried)
        transient_indicators = [
            'connection', 'timeout', 'network', 'temporary',
            'resource temporarily unavailable', 'try again'
        ]
        
        # Fatal errors (should not be retried)
        fatal_indicators = [
            'permission denied', 'no such file or directory',
            'invalid format', 'corrupted', 'out of memory',
            'disk full', 'validation error'
        ]
        
        for indicator in transient_indicators:
            if indicator in error_str:
                return ErrorType.TRANSIENT
        
        for indicator in fatal_indicators:
            if indicator in error_str:
                return ErrorType.FATAL
        
        # Default to recoverable for unknown errors
        return ErrorType.RECOVERABLE
    
    def create_checkpoint(self, step_id: str):
        """
        Create a checkpoint at a key milestone.
        
        Args:
            step_id: The step to create a checkpoint for
        """
        try:
            self.state.checkpoints[step_id] = datetime.now()
            self._save_state()
            logger.info(f"Checkpoint created for step {step_id}")
        except Exception as e:
            logger.error(f"Failed to create checkpoint for step {step_id}: {e}")
    
    def rollback_step(self, step_id: str) -> bool:
        """
        Rollback a failed step by cleaning up its outputs.
        
        Args:
            step_id: The step to rollback
            
        Returns:
            True if rollback successful, False otherwise
        """
        try:
            data_path = Path(self.data_dir)
            
            # Define output files/directories for each step
            step_outputs = {
                'pre_chunking_eda': [
                    'reports/pre_chunking_eda_*.md',
                    'reports/pre_chunking_eda_*.json'
                ],
                'doc_conversion': [
                    'train/xml',
                    'test/xml',
                    'reports/doc_conversion_*.md',
                    'reports/doc_conversion_*.json'
                ],
                'document_parsing': [
                    'train/parsed',
                    'test/parsed',
                    'reports/document_parsing_*.md',
                    'reports/document_parsing_*.json'
                ],
                'semantic_chunking': [
                    'chunks_for_embedding.pkl',
                    'reports/semantic_chunking_*.md',
                    'reports/semantic_chunking_*.json'
                ]
            }
            
            if step_id not in step_outputs:
                logger.warning(f"No rollback configuration for step {step_id}")
                return True
            
            # Clean up output files/directories
            for output_pattern in step_outputs[step_id]:
                if '*' in output_pattern:
                    # Handle glob patterns
                    for path in Path('.').glob(output_pattern):
                        if path.exists():
                            if path.is_dir():
                                shutil.rmtree(path)
                            else:
                                path.unlink()
                            logger.info(f"Removed {path} during rollback")
                else:
                    # Handle direct paths
                    full_path = data_path / output_pattern if not Path(output_pattern).is_absolute() else Path(output_pattern)
                    if full_path.exists():
                        if full_path.is_dir():
                            shutil.rmtree(full_path)
                        else:
                            full_path.unlink()
                        logger.info(f"Removed {full_path} during rollback")
            
            # Update step metadata
            if step_id in self.step_metadata:
                self.step_metadata[step_id].status = StepStatus.NOT_STARTED
                self.step_metadata[step_id].error_message = None
                self.step_metadata[step_id].error_type = None
                self.step_metadata[step_id].retry_count = 0
            
            # Update state
            if step_id in self.state.completed_steps:
                self.state.completed_steps.remove(step_id)
            if step_id in self.state.failed_steps:
                self.state.failed_steps.remove(step_id)
            
            self._save_state()
            logger.info(f"Rollback completed for step {step_id}")
            return True
            
        except Exception as e:
            logger.error(f"Error during rollback of step {step_id}: {e}")
            return False
    
    # ================== RESOURCE MONITORING METHODS ==================
    
    def monitor_resources(self) -> Dict[str, Any]:
        """
        Monitor current resource usage.
        
        Returns:
            Dictionary containing resource usage metrics
        """
        try:
            # Get system resource usage
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            disk = psutil.disk_usage('/')
            
            # Get process-specific usage
            process = psutil.Process()
            process_memory = process.memory_info()
            
            resources = {
                'cpu_percent': cpu_percent,
                'memory_percent': memory.percent,
                'memory_used_gb': memory.used / (1024**3),
                'memory_total_gb': memory.total / (1024**3),
                'disk_percent': disk.percent,
                'disk_used_gb': disk.used / (1024**3),
                'disk_total_gb': disk.total / (1024**3),
                'process_memory_mb': process_memory.rss / (1024**2),
                'process_memory_percent': process.memory_percent(),
                'timestamp': datetime.now()
            }
            
            # Check for resource alerts
            self._check_resource_thresholds(resources)
            
            return resources
            
        except Exception as e:
            logger.error(f"Error monitoring resources: {e}")
            return {}
    
    def _check_resource_thresholds(self, resources: Dict[str, Any]):
        """
        Check if resource usage exceeds thresholds and create alerts.
        
        Args:
            resources: Current resource usage metrics
        """
        alerts = []
        
        # Check memory threshold
        if resources.get('memory_percent', 0) > self.resource_thresholds['memory_percent']:
            alert = ResourceAlert(
                alert_type='memory',
                threshold=self.resource_thresholds['memory_percent'],
                current_value=resources['memory_percent'],
                message=f"Memory usage ({resources['memory_percent']:.1f}%) exceeds threshold ({self.resource_thresholds['memory_percent']:.1f}%)",
                timestamp=datetime.now()
            )
            alerts.append(alert)
        
        # Check disk threshold
        if resources.get('disk_percent', 0) > self.resource_thresholds['disk_percent']:
            alert = ResourceAlert(
                alert_type='disk',
                threshold=self.resource_thresholds['disk_percent'],
                current_value=resources['disk_percent'],
                message=f"Disk usage ({resources['disk_percent']:.1f}%) exceeds threshold ({self.resource_thresholds['disk_percent']:.1f}%)",
                timestamp=datetime.now()
            )
            alerts.append(alert)
        
        # Check CPU threshold
        if resources.get('cpu_percent', 0) > self.resource_thresholds['cpu_percent']:
            alert = ResourceAlert(
                alert_type='cpu',
                threshold=self.resource_thresholds['cpu_percent'],
                current_value=resources['cpu_percent'],
                message=f"CPU usage ({resources['cpu_percent']:.1f}%) exceeds threshold ({self.resource_thresholds['cpu_percent']:.1f}%)",
                timestamp=datetime.now()
            )
            alerts.append(alert)
        
        # Log alerts and add to state
        for alert in alerts:
            logger.warning(alert.message)
            self.state.resource_alerts.append(alert)
    
    def get_file_sizes(self, file_paths: List[str]) -> Dict[str, float]:
        """
        Get file sizes for performance tracking.
        
        Args:
            file_paths: List of file paths to check
            
        Returns:
            Dictionary mapping file paths to sizes in bytes
        """
        file_sizes = {}
        
        for file_path in file_paths:
            try:
                path = Path(file_path)
                if path.exists():
                    if path.is_file():
                        file_sizes[file_path] = path.stat().st_size
                    elif path.is_dir():
                        # Calculate directory size
                        total_size = sum(f.stat().st_size for f in path.rglob('*') if f.is_file())
                        file_sizes[file_path] = total_size
            except Exception as e:
                logger.warning(f"Error getting size for {file_path}: {e}")
                file_sizes[file_path] = 0
        
        return file_sizes
    
    def calculate_throughput(self, items_processed: int, duration: float) -> float:
        """
        Calculate processing throughput.
        
        Args:
            items_processed: Number of items processed
            duration: Duration in seconds
            
        Returns:
            Throughput in items per second
        """
        if duration <= 0:
            return 0.0
        return items_processed / duration
    
    def generate_performance_recommendations(self, metrics: PerformanceMetrics) -> List[str]:
        """
        Generate performance recommendations based on metrics.
        
        Args:
            metrics: Performance metrics for analysis
            
        Returns:
            List of performance recommendations
        """
        recommendations = []
        
        # Memory recommendations
        if metrics.memory_usage.get('memory_percent', 0) > 80:
            recommendations.append("Consider increasing available memory or processing data in smaller batches")
        
        # CPU recommendations
        if metrics.cpu_usage > 80:
            recommendations.append("High CPU usage detected - consider parallel processing or optimizing algorithms")
        
        # Disk recommendations
        if metrics.disk_usage.get('disk_percent', 0) > 85:
            recommendations.append("Disk space is running low - consider cleaning up temporary files")
        
        # Throughput recommendations
        if metrics.throughput and metrics.throughput < 1.0:
            recommendations.append("Low processing throughput - consider optimizing data processing pipeline")
        
        # Execution time recommendations
        if metrics.execution_time > 3600:  # 1 hour
            recommendations.append("Step execution time is high - consider implementing checkpointing for long-running processes")
        
        return recommendations
    
    def cleanup_temporary_files(self):
        """Clean up temporary files to free disk space."""
        try:
            temp_patterns = [
                'tmp/*',
                '*.tmp',
                'temp/*',
                '.cache/*',
                '__pycache__/*'
            ]
            
            cleaned_files = []
            for pattern in temp_patterns:
                for path in Path('.').glob(pattern):
                    if path.exists():
                        if path.is_dir():
                            shutil.rmtree(path)
                        else:
                            path.unlink()
                        cleaned_files.append(str(path))
            
            if cleaned_files:
                logger.info(f"Cleaned up temporary files: {cleaned_files}")
            
        except Exception as e:
            logger.error(f"Error cleaning up temporary files: {e}")

    def validate_step_prerequisites(self, step_id: str) -> bool:
        """
        Validate that all prerequisites for a step are met.
        
        Args:
            step_id: The step to validate
            
        Returns:
            True if all prerequisites are met, False otherwise
        """
        if step_id not in self.STEP_DEFINITIONS:
            logger.error(f"Unknown step: {step_id}")
            return False
        
        dependencies = self.STEP_DEFINITIONS[step_id]["dependencies"]
        for dep in dependencies:
            if dep not in self.state.completed_steps:
                logger.error(f"Step {step_id} requires {dep} to be completed first")
                return False
        
        return True

    def get_step_order(self) -> List[str]:
        """Get the correct order of steps based on dependencies."""
        ordered_steps = []
        remaining_steps = set(self.STEP_DEFINITIONS.keys())
        
        while remaining_steps:
            # Find steps with no unmet dependencies
            ready_steps = []
            for step_id in remaining_steps:
                deps = self.STEP_DEFINITIONS[step_id]["dependencies"]
                if all(dep in ordered_steps for dep in deps):
                    ready_steps.append(step_id)
            
            if not ready_steps:
                # Circular dependency or other issue
                logger.error("Cannot resolve step dependencies")
                break
            
            # Sort ready steps to maintain consistent order
            ready_steps.sort()
            ordered_steps.extend(ready_steps)
            remaining_steps -= set(ready_steps)
        
        return ordered_steps

    def update_step_parameters(self, step_id: str, parameters: Dict[str, Any]):
        """Update parameters for a specific step."""
        if step_id not in self.step_parameters:
            logger.error(f"Unknown step: {step_id}")
            return
        
        self.step_parameters[step_id].update(parameters)
        logger.info(f"Updated parameters for step {step_id}")

    def _execute_step_method(self, step_id: str) -> Dict[str, Any]:
        """Execute the method for a specific step."""
        step_def = self.STEP_DEFINITIONS[step_id]
        method_name = step_def["method"]
        
        if not hasattr(self, method_name):
            raise AttributeError(f"Method {method_name} not found for step {step_id}")
        
        method = getattr(self, method_name)
        parameters = self.step_parameters[step_id]
        
        # Execute the method with parameters
        if method_name in ["pre_chunking_eda", "doc_conversion", "document_parsing"]:
            return method(**parameters)
        elif method_name == "semantic_chunking":
            return method(**parameters)
        else:
            return method()

    def run_single_step(self, step_id: str, force: bool = False) -> bool:
        """
        Run a single preprocessing step with enhanced validation, monitoring, and error handling.
        
        Args:
            step_id: The step to run
            force: If True, run even if already completed
            
        Returns:
            True if successful, False otherwise
        """
        if step_id not in self.STEP_DEFINITIONS:
            logger.error(f"Unknown step: {step_id}")
            return False
        
        # Check if already completed
        if not force and step_id in self.state.completed_steps:
            logger.info(f"Step {step_id} already completed, skipping")
            return True
        
        # Validate prerequisites
        if not self.validate_step_prerequisites(step_id):
            return False
        
        # Enhanced validation before starting
        logger.info(f"Validating prerequisites for step {step_id}")
        
        # Validate data directory structure
        if not self.validate_data_directory_structure():
            logger.error(f"Data directory validation failed for step {step_id}")
            return False
        
        # Validate file dependencies
        if not self.validate_file_dependencies(step_id):
            logger.error(f"File dependency validation failed for step {step_id}")
            return False
        
        # Validate input format
        if not self.validate_step_input_format(step_id):
            logger.error(f"Input format validation failed for step {step_id}")
            return False
        
        # Get current step metadata
        metadata = self.step_metadata[step_id]
        
        # Retry logic for failed steps
        max_retries = self.max_retries
        retry_count = metadata.retry_count
        
        for attempt in range(retry_count, max_retries + 1):
            try:
                # Clean up temporary files before starting
                self.cleanup_temporary_files()
                
                # Update step status
                metadata.status = StepStatus.RUNNING
                metadata.start_time = datetime.now()
                metadata.parameters = self.step_parameters[step_id].copy()
                metadata.retry_count = attempt
                
                self.state.current_step = step_id
                self._save_state()
                
                if attempt > 0:
                    logger.info(f"Retrying step: {metadata.name} (attempt {attempt + 1}/{max_retries + 1})")
                else:
                    logger.info(f"Starting step: {metadata.name}")
                
                # Monitor resources before execution
                initial_resources = self.monitor_resources()
                
                # Track input file sizes
                input_files = self._get_step_input_files(step_id)
                input_file_sizes = self.get_file_sizes(input_files)
                
                # Execute the step
                result = self._execute_step_method(step_id)
                
                # Monitor resources after execution
                final_resources = self.monitor_resources()
                
                # Track output file sizes
                output_files = self._get_step_output_files(step_id)
                output_file_sizes = self.get_file_sizes(output_files)
                
                # Calculate performance metrics
                metadata.end_time = datetime.now()
                metadata.duration = (metadata.end_time - metadata.start_time).total_seconds()
                
                # Create performance metrics
                performance_metrics = PerformanceMetrics(
                    execution_time=metadata.duration,
                    memory_usage={
                        'initial_memory_percent': initial_resources.get('memory_percent', 0),
                        'final_memory_percent': final_resources.get('memory_percent', 0),
                        'memory_peak_mb': final_resources.get('process_memory_mb', 0)
                    },
                    cpu_usage=final_resources.get('cpu_percent', 0),
                    disk_usage={
                        'initial_disk_percent': initial_resources.get('disk_percent', 0),
                        'final_disk_percent': final_resources.get('disk_percent', 0)
                    },
                    file_sizes={
                        'input_files': input_file_sizes,
                        'output_files': output_file_sizes
                    }
                )
                
                # Calculate throughput if applicable
                if isinstance(result, dict) and 'items_processed' in result:
                    performance_metrics.throughput = self.calculate_throughput(
                        result['items_processed'], metadata.duration
                    )
                
                # Update metadata on success
                metadata.status = StepStatus.COMPLETED
                metadata.metrics = result if isinstance(result, dict) else {}
                metadata.performance_metrics = performance_metrics
                metadata.error_message = None
                metadata.error_type = None
                
                # Generate performance recommendations
                recommendations = self.generate_performance_recommendations(performance_metrics)
                if recommendations:
                    logger.info(f"Performance recommendations for {step_id}: {recommendations}")
                
                # Update state
                if step_id not in self.state.completed_steps:
                    self.state.completed_steps.append(step_id)
                if step_id in self.state.failed_steps:
                    self.state.failed_steps.remove(step_id)
                
                self.state.current_step = None
                
                # Create checkpoint
                self.create_checkpoint(step_id)
                
                self._save_state()
                
                logger.info(f"Step {metadata.name} completed successfully in {metadata.duration:.2f} seconds")
                return True
                
            except Exception as e:
                # Categorize the error
                error_type = self.categorize_error(e)
                
                # Update metadata on failure
                metadata.end_time = datetime.now()
                metadata.duration = (metadata.end_time - metadata.start_time).total_seconds()
                metadata.status = StepStatus.FAILED
                metadata.error_message = str(e)
                metadata.error_type = error_type
                
                logger.error(f"Step {metadata.name} failed (attempt {attempt + 1}/{max_retries + 1}): {e}")
                
                # Decide whether to retry based on error type
                if error_type == ErrorType.FATAL:
                    logger.error(f"Fatal error in step {step_id}, not retrying")
                    break
                elif error_type == ErrorType.TRANSIENT and attempt < max_retries:
                    logger.info(f"Transient error in step {step_id}, will retry after {self.retry_delay} seconds")
                    time.sleep(self.retry_delay)
                    continue
                elif error_type == ErrorType.RECOVERABLE and attempt < max_retries:
                    logger.info(f"Recoverable error in step {step_id}, attempting rollback and retry")
                    self.rollback_step(step_id)
                    time.sleep(self.retry_delay)
                    continue
                else:
                    logger.error(f"Max retries exceeded for step {step_id}")
                    break
        
        # If we get here, the step failed after all retries
        # Update state
        if step_id not in self.state.failed_steps:
            self.state.failed_steps.append(step_id)
        if step_id in self.state.completed_steps:
            self.state.completed_steps.remove(step_id)
        
        self.state.current_step = None
        self._save_state()
        
        return False
    
    def _get_step_input_files(self, step_id: str) -> List[str]:
        """
        Get list of input files for a step.
        
        Args:
            step_id: The step to get input files for
            
        Returns:
            List of input file paths
        """
        data_path = Path(self.data_dir)
        
        input_files = {
            'pre_chunking_eda': [
                str(data_path / 'train/documents'),
                str(data_path / 'test/documents')
            ],
            'doc_conversion': [
                str(data_path / 'train/documents'),
                str(data_path / 'test/documents')
            ],
            'document_parsing': [
                str(data_path / 'train/xml'),
                str(data_path / 'test/xml')
            ],
            'semantic_chunking': [
                str(data_path / 'train/parsed/parsed_documents.pkl')
            ],
            'vector_embeddings': [
                'chunks_for_embedding.pkl'
            ],
            'chunk_level_eda': [
                'chunks_for_embedding.pkl'
            ],
            'qc': [
                'chunks_for_embedding.pkl'
            ],
            'export_artifacts': [
                'chunks_for_embedding.pkl'
            ]
        }
        
        return input_files.get(step_id, [])
    
    def _get_step_output_files(self, step_id: str) -> List[str]:
        """
        Get list of output files for a step.
        
        Args:
            step_id: The step to get output files for
            
        Returns:
            List of output file paths
        """
        data_path = Path(self.data_dir)
        
        output_files = {
            'pre_chunking_eda': [
                'reports'
            ],
            'doc_conversion': [
                str(data_path / 'train/xml'),
                str(data_path / 'test/xml'),
                'reports'
            ],
            'document_parsing': [
                str(data_path / 'train/parsed'),
                str(data_path / 'test/parsed'),
                'reports'
            ],
            'semantic_chunking': [
                'chunks_for_embedding.pkl',
                'reports'
            ],
            'vector_embeddings': [
                'vector_embeddings.pkl',
                'reports'
            ],
            'chunk_level_eda': [
                'reports'
            ],
            'qc': [
                'reports'
            ],
            'export_artifacts': [
                'artifacts',
                'reports'
            ]
        }
        
        return output_files.get(step_id, [])

    def run_specific_steps(self, steps: List[str], force: bool = False) -> bool:
        """
        Run specific preprocessing steps.
        
        Args:
            steps: List of step IDs to run
            force: If True, run even if already completed
            
        Returns:
            True if all steps succeeded, False otherwise
        """
        # Validate all steps exist
        for step_id in steps:
            if step_id not in self.STEP_DEFINITIONS:
                logger.error(f"Unknown step: {step_id}")
                return False
        
        # Get proper execution order
        ordered_steps = self.get_step_order()
        steps_to_run = [step for step in ordered_steps if step in steps]
        
        logger.info(f"Running specific steps: {steps_to_run}")
        
        success = True
        for step_id in steps_to_run:
            if not self.run_single_step(step_id, force):
                success = False
                logger.error(f"Failed to execute step {step_id}")
                break
        
        return success

    def run_up_to_step(self, target_step: str, force: bool = False) -> bool:
        """
        Run all steps up to and including the target step.
        
        Args:
            target_step: The final step to run
            force: If True, run even if already completed
            
        Returns:
            True if all steps succeeded, False otherwise
        """
        if target_step not in self.STEP_DEFINITIONS:
            logger.error(f"Unknown step: {target_step}")
            return False
        
        # Get all steps up to target
        ordered_steps = self.get_step_order()
        try:
            target_index = ordered_steps.index(target_step)
            steps_to_run = ordered_steps[:target_index + 1]
        except ValueError:
            logger.error(f"Step {target_step} not found in step order")
            return False
        
        logger.info(f"Running steps up to {target_step}: {steps_to_run}")
        
        success = True
        for step_id in steps_to_run:
            if not self.run_single_step(step_id, force):
                success = False
                logger.error(f"Failed to execute step {step_id}")
                break
        
        return success

    def run_from_step(self, start_step: str, force: bool = False) -> bool:
        """
        Run all steps starting from the specified step.
        
        Args:
            start_step: The first step to run
            force: If True, run even if already completed
            
        Returns:
            True if all steps succeeded, False otherwise
        """
        if start_step not in self.STEP_DEFINITIONS:
            logger.error(f"Unknown step: {start_step}")
            return False
        
        # Get all steps from start step
        ordered_steps = self.get_step_order()
        try:
            start_index = ordered_steps.index(start_step)
            steps_to_run = ordered_steps[start_index:]
        except ValueError:
            logger.error(f"Step {start_step} not found in step order")
            return False
        
        logger.info(f"Running steps from {start_step}: {steps_to_run}")
        
        success = True
        for step_id in steps_to_run:
            if not self.run_single_step(step_id, force):
                success = False
                logger.error(f"Failed to execute step {step_id}")
                break
        
        return success

    def resume_pipeline(self, force: bool = False) -> bool:
        """
        Resume pipeline from the last successful step.
        
        Args:
            force: If True, re-run the last failed step
            
        Returns:
            True if successful, False otherwise
        """
        if not self.state.completed_steps:
            logger.info("No completed steps found, starting from beginning")
            return self.run_all()
        
        # Find the next step to run
        ordered_steps = self.get_step_order()
        completed_set = set(self.state.completed_steps)
        
        next_step_index = None
        for i, step_id in enumerate(ordered_steps):
            if step_id not in completed_set:
                next_step_index = i
                break
        
        if next_step_index is None:
            logger.info("All steps are already completed")
            return True
        
        next_step = ordered_steps[next_step_index]
        logger.info(f"Resuming pipeline from step: {next_step}")
        
        return self.run_from_step(next_step, force)

    def run_all(self, force: bool = False) -> bool:
        """
        Run all preprocessing steps in order.
        
        Args:
            force: If True, run even if already completed
            
        Returns:
            True if all steps succeeded, False otherwise
        """
        ordered_steps = self.get_step_order()
        logger.info(f"Running all preprocessing steps: {ordered_steps}")
        
        self.state.start_time = datetime.now()
        self._save_state()
        
        success = True
        for step_id in ordered_steps:
            if not self.run_single_step(step_id, force):
                success = False
                logger.error(f"Failed to execute step {step_id}")
                break
        
        # Generate consolidated report
        if success:
            self.generate_consolidated_reports()
        
        return success

    def generate_consolidated_json_summary(self) -> Dict[str, Any]:
        """Generate consolidated JSON summary of the pipeline."""
        summary = {
            "pipeline_summary": {
                "total_steps": len(self.STEP_DEFINITIONS),
                "completed_steps": len(self.state.completed_steps),
                "failed_steps": len(self.state.failed_steps),
                "success_rate": len(self.state.completed_steps) / len(self.STEP_DEFINITIONS) * 100,
                "pipeline_start_time": self.state.start_time.isoformat() if self.state.start_time else None,
                "last_update": self.state.last_update.isoformat() if self.state.last_update else None,
                "total_retries": sum(metadata.retry_count for metadata in self.step_metadata.values()),
                "resource_alerts_count": len(self.state.resource_alerts)
            },
            "step_details": {},
            "performance_summary": self._generate_performance_summary(),
            "resource_alerts": self._serialize_resource_alerts(),
            "cross_step_analysis": self._generate_cross_step_analysis(),
            "generated_at": datetime.now().isoformat()
        }
        
        # Add individual step details using Pydantic's model_dump
        for step_id, metadata in self.step_metadata.items():
            metadata_dict = metadata.model_dump()
            # Convert enum to string values
            metadata_dict["status"] = metadata.status.value
            if metadata_dict["error_type"]:
                metadata_dict["error_type"] = metadata.error_type.value
            
            # Convert datetime objects to ISO format
            if metadata_dict["start_time"]:
                metadata_dict["start_time"] = metadata.start_time.isoformat()
            if metadata_dict["end_time"]:
                metadata_dict["end_time"] = metadata.end_time.isoformat()
            
            # Handle performance metrics
            if metadata_dict["performance_metrics"]:
                perf_dict = metadata_dict["performance_metrics"]
                # No datetime conversions needed for performance metrics
            
            # Handle resource alerts
            if metadata_dict["resource_alerts"]:
                for alert in metadata_dict["resource_alerts"]:
                    if alert["timestamp"]:
                        alert["timestamp"] = alert["timestamp"].isoformat()
            
            summary["step_details"][step_id] = metadata_dict
        
        return summary
    
    def _generate_performance_summary(self) -> Dict[str, Any]:
        """Generate performance summary across all steps."""
        completed_steps = [
            metadata for metadata in self.step_metadata.values()
            if metadata.status == StepStatus.COMPLETED and metadata.performance_metrics
        ]
        
        if not completed_steps:
            return {"message": "No completed steps with performance metrics"}
        
        # Calculate aggregate metrics
        total_execution_time = sum(step.performance_metrics.execution_time for step in completed_steps)
        avg_execution_time = total_execution_time / len(completed_steps)
        
        memory_usage = []
        cpu_usage = []
        
        for step in completed_steps:
            if step.performance_metrics.memory_usage:
                memory_usage.append(step.performance_metrics.memory_usage.get('final_memory_percent', 0))
            if step.performance_metrics.cpu_usage:
                cpu_usage.append(step.performance_metrics.cpu_usage)
        
        performance_summary = {
            "total_execution_time": total_execution_time,
            "average_execution_time": avg_execution_time,
            "longest_step": max(completed_steps, key=lambda x: x.performance_metrics.execution_time).name,
            "shortest_step": min(completed_steps, key=lambda x: x.performance_metrics.execution_time).name,
            "memory_usage": {
                "average_percent": sum(memory_usage) / len(memory_usage) if memory_usage else 0,
                "peak_percent": max(memory_usage) if memory_usage else 0
            },
            "cpu_usage": {
                "average_percent": sum(cpu_usage) / len(cpu_usage) if cpu_usage else 0,
                "peak_percent": max(cpu_usage) if cpu_usage else 0
            }
        }
        
        return performance_summary
    
    def _serialize_resource_alerts(self) -> List[Dict[str, Any]]:
        """Serialize resource alerts for JSON output."""
        alerts = []
        
        # Add pipeline-level alerts
        for alert in self.state.resource_alerts:
            alert_dict = alert.model_dump()
            alert_dict["timestamp"] = alert.timestamp.isoformat()
            alerts.append(alert_dict)
        
        # Add step-level alerts
        for step_id, metadata in self.step_metadata.items():
            for alert in metadata.resource_alerts:
                alert_dict = alert.model_dump()
                alert_dict["timestamp"] = alert.timestamp.isoformat()
                alert_dict["step_id"] = step_id
                alerts.append(alert_dict)
        
        return alerts

    def generate_consolidated_markdown_report(self) -> str:
        """Generate consolidated markdown report of the pipeline."""
        summary = self.generate_consolidated_json_summary()
        
        report = f"""# Preprocessing Pipeline Report

**Generated:** {summary['generated_at']}

## Executive Summary

- **Total Steps:** {summary['pipeline_summary']['total_steps']}
- **Completed Steps:** {summary['pipeline_summary']['completed_steps']}
- **Failed Steps:** {summary['pipeline_summary']['failed_steps']}
- **Success Rate:** {summary['pipeline_summary']['success_rate']:.1f}%
- **Total Retries:** {summary['pipeline_summary']['total_retries']}
- **Resource Alerts:** {summary['pipeline_summary']['resource_alerts_count']}

"""
        
        if summary['pipeline_summary']['pipeline_start_time']:
            report += f"- **Pipeline Started:** {summary['pipeline_summary']['pipeline_start_time']}\n"
        
        if summary['pipeline_summary']['last_update']:
            report += f"- **Last Update:** {summary['pipeline_summary']['last_update']}\n"
        
        # Add performance summary
        if summary['performance_summary'].get('total_execution_time'):
            report += f"\n## Performance Summary\n\n"
            perf = summary['performance_summary']
            report += f"- **Total Execution Time:** {perf['total_execution_time']:.2f} seconds\n"
            report += f"- **Average Execution Time:** {perf['average_execution_time']:.2f} seconds\n"
            report += f"- **Longest Step:** {perf['longest_step']}\n"
            report += f"- **Shortest Step:** {perf['shortest_step']}\n"
            report += f"- **Average Memory Usage:** {perf['memory_usage']['average_percent']:.1f}%\n"
            report += f"- **Peak Memory Usage:** {perf['memory_usage']['peak_percent']:.1f}%\n"
            report += f"- **Average CPU Usage:** {perf['cpu_usage']['average_percent']:.1f}%\n"
            report += f"- **Peak CPU Usage:** {perf['cpu_usage']['peak_percent']:.1f}%\n"
        
        # Add resource alerts
        if summary['resource_alerts']:
            report += f"\n## Resource Alerts\n\n"
            for alert in summary['resource_alerts']:
                alert_emoji = {
                    'memory': '🧠',
                    'disk': '💾',
                    'cpu': '⚡'
                }.get(alert['alert_type'], '⚠️')
                
                report += f"### {alert_emoji} {alert['alert_type'].title()} Alert\n\n"
                report += f"- **Message:** {alert['message']}\n"
                report += f"- **Timestamp:** {alert['timestamp']}\n"
                report += f"- **Threshold:** {alert['threshold']:.1f}%\n"
                report += f"- **Current Value:** {alert['current_value']:.1f}%\n"
                if alert.get('step_id'):
                    report += f"- **Step:** {alert['step_id']}\n"
                report += "\n"
        
        report += "\n## Step Details\n\n"
        
        # Add step details
        for step_id, details in summary['step_details'].items():
            status_emoji = {
                'completed': '✅',
                'failed': '❌',
                'running': '🔄',
                'not_started': '⏸️'
            }.get(details['status'], '❓')
            
            report += f"### {status_emoji} {details['name']}\n\n"
            report += f"- **Description:** {details['description']}\n"
            report += f"- **Status:** {details['status']}\n"
            
            if details['duration']:
                report += f"- **Duration:** {details['duration']:.2f} seconds\n"
            
            if details['retry_count'] > 0:
                report += f"- **Retries:** {details['retry_count']}\n"
            
            if details['error_message']:
                report += f"- **Error:** {details['error_message']}\n"
                if details['error_type']:
                    report += f"- **Error Type:** {details['error_type']}\n"
            
            # Add performance metrics
            if details['performance_metrics']:
                perf = details['performance_metrics']
                report += f"- **Performance Metrics:**\n"
                report += f"  - Execution Time: {perf['execution_time']:.2f} seconds\n"
                
                if perf['memory_usage']:
                    report += f"  - Memory Usage: {perf['memory_usage']['final_memory_percent']:.1f}%\n"
                
                if perf['cpu_usage']:
                    report += f"  - CPU Usage: {perf['cpu_usage']:.1f}%\n"
                
                if perf['throughput']:
                    report += f"  - Throughput: {perf['throughput']:.2f} items/second\n"
                
                if perf['file_sizes']:
                    report += f"  - Input Files: {len(perf['file_sizes'].get('input_files', {}))}\n"
                    report += f"  - Output Files: {len(perf['file_sizes'].get('output_files', {}))}\n"
            
            if details['parameters']:
                report += f"- **Parameters:** {json.dumps(details['parameters'], indent=2)}\n"
            
            if details['metrics']:
                report += f"- **Metrics:** {json.dumps(details['metrics'], indent=2)}\n"
            
            report += "\n"
        
        # Add cross-step analysis
        if summary['cross_step_analysis']:
            report += "## Cross-Step Analysis\n\n"
            for analysis_key, analysis_data in summary['cross_step_analysis'].items():
                report += f"### {analysis_key.replace('_', ' ').title()}\n\n"
                report += f"{json.dumps(analysis_data, indent=2)}\n\n"
        
        return report

    def _generate_cross_step_analysis(self) -> Dict[str, Any]:
        """Generate cross-step analysis data."""
        analysis = {}
        
        # Calculate total pipeline duration
        completed_steps = [
            metadata for metadata in self.step_metadata.values()
            if metadata.status == StepStatus.COMPLETED and metadata.duration
        ]
        
        if completed_steps:
            total_duration = sum(step.duration for step in completed_steps)
            analysis["total_pipeline_duration"] = total_duration
            analysis["average_step_duration"] = total_duration / len(completed_steps)
            analysis["longest_step"] = max(completed_steps, key=lambda x: x.duration).name
            analysis["shortest_step"] = min(completed_steps, key=lambda x: x.duration).name
        
        # Add step flow analysis
        analysis["step_flow"] = {
            "completed_steps": self.state.completed_steps,
            "failed_steps": self.state.failed_steps,
            "execution_order": self.get_step_order()
        }
        
        return analysis

    def generate_consolidated_reports(self):
        """Generate and save consolidated reports."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        reports_dir = Path("reports")
        reports_dir.mkdir(exist_ok=True)
        
        # Generate JSON summary
        json_summary = self.generate_consolidated_json_summary()
        json_file = reports_dir / f"preprocessing_pipeline_{timestamp}.json"
        with open(json_file, 'w') as f:
            json.dump(json_summary, f, indent=2)
        
        # Generate Markdown report
        markdown_report = self.generate_consolidated_markdown_report()
        markdown_file = reports_dir / f"preprocessing_pipeline_{timestamp}.md"
        with open(markdown_file, 'w') as f:
            f.write(markdown_report)
        
        # Update report index
        self._update_report_index(json_file, markdown_file)
        
        logger.info(f"Consolidated reports generated: {json_file}, {markdown_file}")

    def _update_report_index(self, json_file: Path, markdown_file: Path):
        """Update the report index file."""
        index_file = Path("reports") / "report_index.md"
        
        index_entry = f"""
## {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
- **JSON Report:** [{json_file.name}]({json_file.name})
- **Markdown Report:** [{markdown_file.name}]({markdown_file.name})
"""
        
        if index_file.exists():
            with open(index_file, 'a') as f:
                f.write(index_entry)
        else:
            with open(index_file, 'w') as f:
                f.write("# Preprocessing Pipeline Reports\n")
                f.write(index_entry)

    # Original step methods (preserved for compatibility)
    def pre_chunking_eda(self, **kwargs):
        """Run pre-chunking EDA step."""
        from scripts.run_prechunking_eda import run_prechunking_eda
        
        # Merge parameters
        params = {
            "data_dir": self.data_dir,
            "output_format": "all",
            "save_reports": True,
            "show_plots": False
        }
        params.update(kwargs)
        
        reporter = run_prechunking_eda(**params)
        return reporter.generate_json_summary()

    def doc_conversion(self, **kwargs):
        """Run document conversion step."""
        from scripts.run_pdf_to_xml_conversion import run_pdf_to_xml_conversion
        
        # Merge parameters
        params = {
            "data_dir": self.data_dir,
            "output_format": "all",
            "save_reports": True
        }
        params.update(kwargs)
        
        reporter = run_pdf_to_xml_conversion(**params)
        return reporter.generate_json_summary()

    def document_parsing(self, **kwargs):
        """Run document parsing step."""
        from scripts.run_full_doc_parsing import run_full_document_parsing
        
        # Merge parameters
        params = {
            "data_dir": self.data_dir,
            "output_format": "all",
            "save_reports": True
        }
        params.update(kwargs)
        
        reporter = run_full_document_parsing(**params)
        return reporter.generate_json_summary()

    def semantic_chunking(self, **kwargs):
        """Run semantic chunking step."""
        from scripts.run_chunking_pipeline import run_semantic_chunking_with_reporting
        
        # Merge parameters
        params = {
            "input_path": f"{self.data_dir}/train/parsed/parsed_documents.pkl",
            "output_path": "chunks_for_embedding.pkl",
            "chunk_size": 200,
            "chunk_overlap": 20,
            "output_format": "all",
            "save_reports": True
        }
        params.update(kwargs)
        
        reporter = run_semantic_chunking_with_reporting(**params)
        return reporter.generate_json_summary()

    def create_vector_embeddings(self):
        """Create vector embeddings (placeholder)."""
        logger.info("Creating vector embeddings - placeholder implementation")
        return {"status": "placeholder", "message": "Vector embeddings step not implemented"}

    def chunk_level_eda(self):
        """Run chunk-level EDA (placeholder)."""
        logger.info("Running chunk-level EDA - placeholder implementation")
        return {"status": "placeholder", "message": "Chunk-level EDA step not implemented"}

    def qc(self):
        """Run quality control (placeholder)."""
        logger.info("Running quality control - placeholder implementation")
        return {"status": "placeholder", "message": "QC step not implemented"}

    def export_artifacts(self):
        """Export artifacts (placeholder)."""
        logger.info("Exporting artifacts - placeholder implementation")
        return {"status": "placeholder", "message": "Export artifacts step not implemented"}


def create_argument_parser():
    """Create comprehensive argument parser for CLI interface."""
    parser = argparse.ArgumentParser(
        description="Preprocessing Pipeline for Make Data Count Challenge",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Run all steps
  python preprocessing.py
  
  # Run specific steps
  python preprocessing.py --steps pre_chunking_eda,doc_conversion
  
  # Run up to a specific step
  python preprocessing.py --up-to semantic_chunking
  
  # Run from a specific step
  python preprocessing.py --from document_parsing
  
  # Resume from last successful step
  python preprocessing.py --resume
  
  # Run with custom parameters
  python preprocessing.py --chunk-size 300 --chunk-overlap 30
        """
    )
    
    # Data directory
    parser.add_argument(
        "--data-dir",
        default="Data",
        help="Directory containing the data (default: Data)"
    )
    
    # Step selection arguments
    parser.add_argument(
        "--steps",
        help="Comma-separated list of specific steps to run"
    )
    
    parser.add_argument(
        "--up-to",
        help="Run all steps up to and including this step"
    )
    
    parser.add_argument(
        "--from",
        dest="from_step",
        help="Run all steps starting from this step"
    )
    
    parser.add_argument(
        "--resume",
        action="store_true",
        help="Resume from the last successful step"
    )
    
    parser.add_argument(
        "--force",
        action="store_true",
        help="Force re-run of completed steps"
    )
    
    # Parameter customization
    parser.add_argument(
        "--chunk-size",
        type=int,
        default=200,
        help="Chunk size for semantic chunking (default: 200)"
    )
    
    parser.add_argument(
        "--chunk-overlap",
        type=int,
        default=20,
        help="Chunk overlap for semantic chunking (default: 20)"
    )
    
    # Output format controls
    parser.add_argument(
        "--output-format",
        choices=["console", "markdown", "json", "all"],
        default="all",
        help="Output format for reports (default: all)"
    )
    
    parser.add_argument(
        "--save-reports",
        action="store_true",
        default=True,
        help="Save reports to files (default: True)"
    )
    
    parser.add_argument(
        "--show-plots",
        action="store_true",
        help="Show plots during EDA steps"
    )
    
    # Debugging and verbose logging
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Enable verbose logging"
    )
    
    parser.add_argument(
        "--debug",
        action="store_true",
        help="Enable debug logging"
    )
    
    # Configuration file
    parser.add_argument(
        "--config",
        help="Path to configuration file"
    )
    
    # List available steps
    parser.add_argument(
        "--list-steps",
        action="store_true",
        help="List all available steps and exit"
    )
    
    return parser


def main():
    """Main entry point for CLI interface."""
    parser = create_argument_parser()
    args = parser.parse_args()
    
    # Configure logging based on arguments
    if args.debug:
        import logging
        logging.getLogger().setLevel(logging.DEBUG)
    elif args.verbose:
        import logging
        logging.getLogger().setLevel(logging.INFO)
    
    # List steps if requested
    if args.list_steps:
        print("Available preprocessing steps:")
        for step_id, step_def in PreprocessingPipeline.STEP_DEFINITIONS.items():
            deps = ", ".join(step_def["dependencies"]) if step_def["dependencies"] else "None"
            print(f"  {step_id}: {step_def['name']}")
            print(f"    Description: {step_def['description']}")
            print(f"    Dependencies: {deps}")
            print()
        return
    
    # Initialize pipeline
    pipeline = PreprocessingPipeline(
        data_dir=args.data_dir,
        config_file=args.config
    )
    
    # Update step parameters based on CLI arguments
    if args.chunk_size or args.chunk_overlap:
        semantic_params = {}
        if args.chunk_size:
            semantic_params["chunk_size"] = args.chunk_size
        if args.chunk_overlap:
            semantic_params["chunk_overlap"] = args.chunk_overlap
        pipeline.update_step_parameters("semantic_chunking", semantic_params)
    
    # Update common parameters
    common_params = {
        "output_format": args.output_format,
        "save_reports": args.save_reports,
        "show_plots": args.show_plots
    }
    
    for step_id in ["pre_chunking_eda", "doc_conversion", "document_parsing", "semantic_chunking"]:
        if step_id in pipeline.step_parameters:
            pipeline.update_step_parameters(step_id, common_params)
    
    # Execute based on arguments
    success = False
    
    try:
        if args.resume:
            success = pipeline.resume_pipeline(force=args.force)
        elif args.steps:
            steps = [s.strip() for s in args.steps.split(",")]
            success = pipeline.run_specific_steps(steps, force=args.force)
        elif args.up_to:
            success = pipeline.run_up_to_step(args.up_to, force=args.force)
        elif args.from_step:
            success = pipeline.run_from_step(args.from_step, force=args.force)
        else:
            success = pipeline.run_all(force=args.force)
        
        if success:
            logger.info("Pipeline execution completed successfully")
        else:
            logger.error("Pipeline execution failed")
            sys.exit(1)
            
    except KeyboardInterrupt:
        logger.info("Pipeline execution interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Pipeline execution failed with error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()