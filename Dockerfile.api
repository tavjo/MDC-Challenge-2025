FROM python:3.12-slim

ENV UV_CACHE_DIR=/tmp/uv-cache \
    PYTHONUNBUFFERED=1 \
    SENTENCE_TRANSFORMERS_HOME=/app/offline_models \
    HF_HOME=/app/offline_models \
    HUGGINGFACE_OFFLINE=1

RUN pip install --no-cache-dir uv

WORKDIR /app

# project metadata
COPY pyproject.toml uv.lock ./

# create venv & install deps (one layer)
RUN python -m venv .venv && . .venv/bin/activate \
 && uv pip install --no-cache-dir torch --index-url https://download.pytorch.org/whl/cpu \
 && uv pip install --no-cache-dir chromadb sentence-transformers accelerate \
                  llama-index llama-index-core \
 && uv sync --locked --inexact

# Install NLTK corpora needed by SentenceSplitter
RUN . .venv/bin/activate \
 && python - <<'PY'
import nltk
nltk.download("punkt")
nltk.download("wordnet")
nltk.download("words")
PY
# -------- download BGE once --------
# RUN . .venv/bin/activate \
#  && python - <<'PY'
# from huggingface_hub import snapshot_download
# snapshot_download(
#     repo_id="BAAI/bge-small-en-v1.5",
#     cache_dir="/app/offline_models"
# )
# PY

# copy code after deps to leverage Docker layer cache
COPY src/ /app/src/
COPY api/ /app/api/
COPY configs/ /app/configs/
COPY tests/ /app/tests/

RUN mkdir -p artifacts logs

ENV PATH="/app/.venv/bin:${PATH}"

CMD ["uvicorn", "api.chunk_and_embed_api:app", "--host", "0.0.0.0", "--port", "8000"]